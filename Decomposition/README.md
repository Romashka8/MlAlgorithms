# Идея

**PCA(Principal Component Analysys)** - метод главных компонент. Один из популярных методов понижения размерности. Представляет собой ортогональное линейное преобразование, которое отображает данные из исходного пространства признаков в новое пространство. А главные компоненты - новые переменные в этом пространстве. После преобразования количество главных компонент равно числу изначальных переменных.

![](https://ucarecdn.com/0f4e4e8e-87b6-4bff-8d19-8e5e5f08d994/)

Но при этом PCA пытается поместить максимум возможной информации (дисперсии) в первый компонент, затем максимум оставшейся информации во второй и т.д.

![](https://ucarecdn.com/f213d78f-8e13-42ab-b047-5c4b14d49c4d/)

И отбросив компоненты с маленькой информацией, мы уменьшим размерность данных с минимальными потерями.

![](https://ucarecdn.com/673be061-b1c9-48d1-a6f1-9db9f37ec720/)

У PCA есть и недостатки:
- PCA ищет только линейные зависимости.
- Полученные главные компоненты менее интерпретируемы (по сравнению с исходными данными).
- Уменьшение размерности приводит к потере информации, и может ухудшить точность модели.
# Алгоритм

PCA можно рассчитать путем разложения на собственные вектора матрицы ковариации или путем разложения на сингулярные значения (SVD). Мы воспользуемся матрицей ковариации:

1) Нормируем исходный набор данных, приводя их к единому масштабу. Для этого из каждого значения вычитаем среднее значение его столбца.

2) Вычисляем матрицу ковариации (на нормированных данных). Ковариационная матрица это квадратная матрица, на диагонали которой располагаются дисперсии векторов, а внедиагональные элементы — ковариации между векторами. Векторами здесь выступают столбцы исходного датасета. Ковариационная матрица позволяет увидеть, есть ли какая-либо связь между фичами.
$$
\Sigma = \begin{pmatrix}
\text{var}(x) & \text{cov}(x,y) & \text{cov}(x,z) \\
\text{cov}(x,y) & \text{var}(y) & \text{cov}(y,z) \\
\text{cov}(x,z) & \text{cov}(y,z) & \text{var}(z)
\end{pmatrix}
$$

3) Делаем разложение матрицы ковариации на собственные вектора и собственные числа. На выходе у вас должно получится столько же пар "собственный вектор-собственное число", сколько фичей в датасете.

Собственные векторы матрицы ковариации являются направлениями осей, где наблюдается наибольшая дисперсия (т.е. раскидана большая часть информации). Эти вектора и называются главными компонентами. А собственные значения — это коэффициенты, которые отражают величину дисперсии.

Теория: пусть задана квадратная матрица $A$. Число $\lambda$ называется собственным числом матрицы $A$, если существует ненулевой вектор $w$, такой что $A w = \lambda w$. При этом вектор $w$ называется собственным вектором матрицы $A$.

4) Отсортируйте собственные значения в порядке убывания вместе с соответствующим им собственными векторами. Чем больше собственное число — тем больше дисперсия. Соответственно, отсортировав их мы получим, что первый собственный вектор будет главным компонентом (который отражает наибольшую изменчивость), второй - вторым компонентом и т.д.

5) Берем $N$ первых собственных векторов. Это и есть искомые главные компоненты.

6) Снижение размерности. Приведем исходные данные в сокращенный формат. Для этого надо перемножить исходные данные с отобранными собственными векторами:
$$
X_{\text{reduced}} = X_{\text{meaned}} W_{\text{pca}}
$$

где:
	$X_{\text{reduced}}$ — уменьшенная матрица фичей
	$X_{\text{meaned}}$ — матрица исходных нормированных фичей
	$W_{\text{pca}}$ — матрица из отобранных топ-$N$ векторов главных компонент